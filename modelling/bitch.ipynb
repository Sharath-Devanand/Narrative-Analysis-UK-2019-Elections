{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f315005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39fc84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Sample:\n",
      "              id                                              tweet  \\\n",
      "0  1.190000e+18  @mention HOPEFULLY NONE ENTITY LOSE SEAT ST GE...   \n",
      "1  1.190000e+18  @mention Jacob baby way forward go alliance Br...   \n",
      "2  1.200000e+18  @mention care climate change amp poverty causi...   \n",
      "3  1.190000e+18  @mention @mention @mention dogs matter rspca p...   \n",
      "4  1.200000e+18  moved canvassing Bomere Heath Fighting ever la...   \n",
      "\n",
      "      annotations.supernarrative_1  Label  \n",
      "0  Political hate and polarisation      9  \n",
      "1                          Anti-EU      1  \n",
      "2         Distrust in institutions      4  \n",
      "3  Political hate and polarisation      9  \n",
      "4                              NaN      0  \n"
     ]
    }
   ],
   "source": [
    "# Load your CSV file\n",
    "file_path = r'Data\\combined_prepros.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "print(\"Initial Data Sample:\\n\", df.head())\n",
    "\n",
    "# Select the data columns for features and labels\n",
    "X = df['tweet'].tolist()\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad803a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfb9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 Training Texts:\n",
      " ['politics differently Come join PeoplesAssembly @mention tomorrow night party politics talked open equal conversation makes strong community Everybody welcome https', 'Cheers signed PledgeforPubs celebrate promote great beer Support action help pubs thrive represent interests pub goers beer cider drinkers @mention ge2019 info visit https', 'Every Christmas day young volunteers St Joseph church Rammy organise meal Bury Parish Church anyone isolated well vulnerable families homeless place go Christmas Day always great day', 'BBC Demand Jo Swinson included TV debates run General Election Sign Petition https via @mention', '@mention said commit @mention deal know']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the first few training examples to ensure they are strings\n",
    "print(\"\\nFirst 5 Training Texts:\\n\", X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c27584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb5aa15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3333a9286ca14585911025f48a282c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  70%|#######   | 189M/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed04f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get DistilBERT embeddings for a list of texts\n",
    "def get_distilbert_embeddings(texts, batch_size=32):\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        # Ensure inputs are a list of strings\n",
    "        if not isinstance(batch_texts, list):\n",
    "            batch_texts = [batch_texts]\n",
    "\n",
    "        # Print the current batch of texts to verify format\n",
    "        #print(\"\\nCurrent Batch of Texts:\\n\", batch_texts)\n",
    "\n",
    "        # Tokenize the input texts directly using the tokenizer object\n",
    "        encoded_inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=144,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Process the inputs with DistilBERT to obtain outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = distilbert_model(**encoded_inputs)\n",
    "\n",
    "        # Extract the embedding of the [CLS] token for classification\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    return np.concatenate(embeddings_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4226c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain DistilBERT embeddings for the training and testing data\n",
    "X_train_embeddings = get_distilbert_embeddings(X_train)\n",
    "X_test_embeddings = get_distilbert_embeddings(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f0798f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize embeddings\n",
    "scaler = StandardScaler()\n",
    "X_train_embeddings = scaler.fit_transform(X_train_embeddings)\n",
    "X_test_embeddings = scaler.transform(X_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb11043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a simple classifier on the DistilBERT embeddings\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73979bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b951e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5375\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62579e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
